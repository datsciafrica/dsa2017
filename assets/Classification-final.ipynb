{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "## Quick Detour, Nearest Neighbours\n",
    "\n",
    "Before we use Scikit Learn (a library for doing machine learning), we'll quickly look at how we might write a nearest neighbour classifier by hand.\n",
    "\n",
    "First we load some data. We have three arrays, x1, x2 with features and y with the corresponding classes of the objects.\n",
    "\n",
    "We will use all but the last object to 'train' the nearest neighbour, and the last object to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#The nutrition data\n",
    "x1=np.array([1,1,1,1,2,2,3,3,4,5,5,6,6,7,7,7,8,8])\n",
    "x2=np.array([4,6,8,9,6,8,6,9,3,2,4,4,5,3,5,6,4,7])\n",
    "y= np.array([1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest neighbour simply involves calculating the **distance** from the test point, to all the training points. We can find the distance of a point $x_1, y_1$ from $x_2, y_2$ by using pythagoras: \n",
    "$$\\mathtt{distance} = \\sqrt{ (x_1-x_2)^2 + (y_1-y_2)^2}$$\n",
    "\n",
    "We can get the test point (the last point in the lists):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_zscore = x1[-1] #in python the -1 index means the end of the list\n",
    "test_muac = x2[-1]\n",
    "test_ok = y[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to loop through all the training inputs, and find the distance from each to the test point: We want to find the training point that's got the smallest distance to the test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The algorithm predicts that the test point has a label: '2'\n",
      "The test point actually has label: '2'\n"
     ]
    }
   ],
   "source": [
    "smallest_distance = np.infty #this variable remembers the smallest distance that we've seen so far.\n",
    "result = np.NAN #this variable remembers the associated label for that training point.\n",
    "\n",
    "for child_zscore, child_muac, child_ok in list(zip(x1,x2,y))[0:-1]: #this means we loop through all but the last child\n",
    "    distance = np.sqrt((child_zscore - test_zscore)**2 + (child_muac - test_muac)**2) #get distance from test to muac\n",
    "    if distance<smallest_distance: #if it's the smallest distance we've seen, use it.\n",
    "        smallest_distance = distance #we update the smallest distance with this one\n",
    "        result = child_ok  #we make a note of the label of this training point\n",
    "        \n",
    "print(\"The algorithm predicts that the test point has a label: '%s'\" % result)\n",
    "print(\"The test point actually has label: '%s'\" % test_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be revisiting this dataset later, but we now turn to the 'digits' dataset and scikit learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Scikit Learn\n",
    "\n",
    "## The Digit Dataset\n",
    "\n",
    "For these classification examples we will be using scikit-learn, a toolkit for python that contains lots of methods for solving machine learning problems.\n",
    "\n",
    "It also contains some datasets we can try out. For this exercise we'll use the 'digit' dataset. This is a set of 1797 pictures of hand-drawn digits (0,1,2,3,4,5,6,7,8,9). The challenge is whether we can get the computer to learn what the digits look like, using a training set of images, and then test the computer on a test-set of images.\n",
    "\n",
    "Run the code below to import the digits dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt #plotting library (lets us draw graphs)\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import datasets #the datasets from sklearn\n",
    "\n",
    "\n",
    "digits = datasets.load_digits() #load the digits into the variable 'digits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sklearn.datasets' from '/home/albert/anaconda2/lib/python2.7/site-packages/sklearn/datasets/__init__.pyc'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the data we are going to be classifying we'll ask what shape the 'data' matrix is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that it has 1797 rows (which are the samples) and 64 columns (which are the 8x8 pixels in the data, and make up the 64 dimensions of the data set).\n",
    "\n",
    "We can have a look at just one sample. Here I'm using python's matrix 'slicing' notation. It means I want row 35 and all the columns from that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   3.,  15.,   8.,   8.,   6.,   0.,   0.,   0.,   4.,  16.,\n",
       "        16.,  16.,  13.,   2.,   0.,   0.,   3.,  16.,   9.,   2.,   0.,\n",
       "         0.,   0.,   0.,   2.,  16.,  16.,  15.,   3.,   0.,   0.,   0.,\n",
       "         0.,   7.,   6.,  12.,   9.,   0.,   0.,   0.,   0.,   0.,   1.,\n",
       "        14.,  10.,   0.,   0.,   0.,   0.,   5.,  14.,  15.,   2.,   0.,\n",
       "         0.,   0.,   1.,  15.,  14.,   1.,   0.,   0.,   0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.data[35,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these numbers is one of the pixels in the image.\n",
    "\n",
    "It's unclear what digit this image is of still.\n",
    "\n",
    "We can draw the numbers as pixels in an image to see what image this represents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xab70370c>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxQcSovhAQhQfSIjiAwn9BxjgwLbqj3IrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xab6bef4c>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#code to reshape the 64 numbers into an 8x8 matrix and then draw it\n",
    "plt.matshow(digits.data[35,:].reshape(8,8),cmap='gray') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a five!\n",
    "\n",
    "**Exercise 1:** Copy the code above into the box below, and modify it to find out what digit image 72 has inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "#Exercise 1: Your code here!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is a **supervised** learning problem, which means we need to provide labels for our data points.\n",
    "\n",
    "The labels are also in the 'digits' object. They can be accessed using 'target':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target[35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected image 35 is of the digit '5'.\n",
    "\n",
    "**Exercise 2**: What is image 72 supposed to be? Copy and alter the code above to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Exercise 2: Your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**: How many digits are of each type in the dataset?\n",
    "\n",
    "You can plot a histogram with the following code:\n",
    "\n",
    "    plt.hist(list_of_numbers,bins=range(11))\n",
    "    \n",
    "You'll need to replace \"list_of_numbers\" with the array you want to draw the histogram of.\n",
    "\n",
    "The bins parameter tells the function where to draw the boundaries of the graph. We want them at each integer.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit Learn\n",
    "In the following we'll be using the scikit library to do the classification. We need to go through two steps: **training** (fitting) and **testing** (prediction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "First we need to pick some training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = digits.data[0:-10,:] #this means all but the last 10 rows should be put in training_data\n",
    "training_target = digits.target[0:-10] #this puts all but the last 10 elements of the labels (targets) into training_target\n",
    "\n",
    "#similarly this takes the last digit and puts that in test_data and test_target\n",
    "test_data = digits.data[-10:,:]\n",
    "test_target = digits.target[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training step is quite simple. Here we fit the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors #import the library that we need\n",
    "nn = neighbors.KNeighborsClassifier(n_neighbors=1) #this is our model (with just one nearest neighbour)\n",
    "nn.fit(training_data,training_target); #fit our model to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then predict the results using the predict method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'auto'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 8, 1, 4, 9, 0, 8, 9, 8])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many of these were correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 8, 8, 4, 9, 0, 8, 9, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarkably the classifier has mostly got them correct.\n",
    "\n",
    "Below is the image that it misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x115354780>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPcAAAD7CAYAAAC2TgIoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x113fc8c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(test_data[3].reshape(8,8),cmap='gray') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**: What was the image supposed to be, and what did the classifier think it was?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Exercise 4: Answer here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "In the lecture I briefly mentioned leave-one-out cross-validation. The same sort of idea is called **k-fold cross-validation**. In this we split the dataset into *k* groups, and train on all but one, then test on the remaining one. Then repeat while leaving out a different group.\n",
    "\n",
    "sklearn provides the 'KFold' object to let us organise our cross-validation. In the code below we repeatedly train and test, and report the accuracy in each fold.\n",
    "\n",
    "**Exercise 5**: In the code below try different classifiers and see which one does best. Try modifying the parameters to see what effect they have (e.g. the number of neighbours). Put the cursor inside the parameter brackets (e.g. on the \"n_neighbors=1\") and press SHIFT-TAB a couple of times and a box will appear describing what the different parameters do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346 of 360 correct\n",
      "343 of 360 correct\n",
      "347 of 359 correct\n",
      "355 of 359 correct\n",
      "343 of 359 correct\n",
      " \n",
      "Total: 1734 of 1797 correct (96.49%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#classification libraries\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "#prepare k-fold cross validation\n",
    "kf = KFold(len(digits.target), n_folds=5)\n",
    "KFold(n=4, n_folds=2, shuffle=False, random_state=None)\n",
    "\n",
    "#variables to count up how many we got right\n",
    "tally_correct = 0\n",
    "tally_total = 0\n",
    "for train_index, test_index in kf:\n",
    "    #here we split the dataset up into training and test sets, these change each iteration\n",
    "    training_data = digits.data[train_index,:] \n",
    "    training_target = digits.target[train_index] \n",
    "    test_data = digits.data[test_index,:]\n",
    "    test_target = digits.target[test_index]\n",
    "    \n",
    "    #TODO: Uncomment one of these classifiers to see how it does\n",
    "    #csf = tree.DecisionTreeClassifier()\n",
    "    #csf = ensemble.RandomForestClassifier(n_estimators=50, min_samples_split=1, max_depth=None, max_features=16)\n",
    "    #csf = ensemble.ExtraTreesClassifier(n_estimators=100, min_samples_split=1, max_depth=None, max_features=8)\n",
    "    csf = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "    #csf= svm.LinearSVC(C=0.05) #Linear Support Vector Machine classifier\n",
    "    #csf = naive_bayes.GaussianNB()\n",
    "    \n",
    "    csf.fit(training_data,training_target)\n",
    "    \n",
    "    predictions = csf.predict(test_data)\n",
    "    number_correct = np.sum(predictions==test_target)\n",
    "    total_number = len(predictions)\n",
    "    print(\"%d of %d correct\" % (number_correct,total_number))\n",
    "    tally_correct += number_correct\n",
    "    tally_total += total_number\n",
    "print(\" \")\n",
    "print(\"Total: %d of %d correct (%0.2f%%)\" % (tally_correct, tally_total, 100.0*tally_correct/tally_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nearest neighbour classifier did particularly well on the digits dataset. \n",
    "\n",
    "### Breast Cancer Dataset\n",
    "\n",
    "This is the wisconsin Breast Cancer dataset. It contains measurements of different cells, some of which are cancerous and some of which are not. It's been organised in the same way as before, with bc.data containing a matrix, each row is a cell, each column a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.02900000e+01,   1.43400000e+01,   1.35100000e+02,\n",
       "         1.29700000e+03,   1.00300000e-01,   1.32800000e-01,\n",
       "         1.98000000e-01,   1.04300000e-01,   1.80900000e-01,\n",
       "         5.88300000e-02,   7.57200000e-01,   7.81300000e-01,\n",
       "         5.43800000e+00,   9.44400000e+01,   1.14900000e-02,\n",
       "         2.46100000e-02,   5.68800000e-02,   1.88500000e-02,\n",
       "         1.75600000e-02,   5.11500000e-03,   2.25400000e+01,\n",
       "         1.66700000e+01,   1.52200000e+02,   1.57500000e+03,\n",
       "         1.37400000e-01,   2.05000000e-01,   4.00000000e-01,\n",
       "         1.62500000e-01,   2.36400000e-01,   7.67800000e-02])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "bc.data[4,:] #data from row number four."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find out more by running this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer Wisconsin (Diagnostic) Database\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 569\n",
      "\n",
      "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
      "\n",
      "    :Attribute Information:\n",
      "        - radius (mean of distances from center to points on the perimeter)\n",
      "        - texture (standard deviation of gray-scale values)\n",
      "        - perimeter\n",
      "        - area\n",
      "        - smoothness (local variation in radius lengths)\n",
      "        - compactness (perimeter^2 / area - 1.0)\n",
      "        - concavity (severity of concave portions of the contour)\n",
      "        - concave points (number of concave portions of the contour)\n",
      "        - symmetry \n",
      "        - fractal dimension (\"coastline approximation\" - 1)\n",
      "        \n",
      "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
      "        largest values) of these features were computed for each image,\n",
      "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
      "        13 is Radius SE, field 23 is Worst Radius.\n",
      "        \n",
      "        - class:\n",
      "                - WDBC-Malignant\n",
      "                - WDBC-Benign\n",
      "\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ===================================== ======= ========\n",
      "                                           Min     Max\n",
      "    ===================================== ======= ========\n",
      "    radius (mean):                         6.981   28.11\n",
      "    texture (mean):                        9.71    39.28\n",
      "    perimeter (mean):                      43.79   188.5\n",
      "    area (mean):                           143.5   2501.0\n",
      "    smoothness (mean):                     0.053   0.163\n",
      "    compactness (mean):                    0.019   0.345\n",
      "    concavity (mean):                      0.0     0.427\n",
      "    concave points (mean):                 0.0     0.201\n",
      "    symmetry (mean):                       0.106   0.304\n",
      "    fractal dimension (mean):              0.05    0.097\n",
      "    radius (standard error):               0.112   2.873\n",
      "    texture (standard error):              0.36    4.885\n",
      "    perimeter (standard error):            0.757   21.98\n",
      "    area (standard error):                 6.802   542.2\n",
      "    smoothness (standard error):           0.002   0.031\n",
      "    compactness (standard error):          0.002   0.135\n",
      "    concavity (standard error):            0.0     0.396\n",
      "    concave points (standard error):       0.0     0.053\n",
      "    symmetry (standard error):             0.008   0.079\n",
      "    fractal dimension (standard error):    0.001   0.03\n",
      "    radius (worst):                        7.93    36.04\n",
      "    texture (worst):                       12.02   49.54\n",
      "    perimeter (worst):                     50.41   251.2\n",
      "    area (worst):                          185.2   4254.0\n",
      "    smoothness (worst):                    0.071   0.223\n",
      "    compactness (worst):                   0.027   1.058\n",
      "    concavity (worst):                     0.0     1.252\n",
      "    concave points (worst):                0.0     0.291\n",
      "    symmetry (worst):                      0.156   0.664\n",
      "    fractal dimension (worst):             0.055   0.208\n",
      "    ===================================== ======= ========\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
      "\n",
      "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
      "\n",
      "    :Donor: Nick Street\n",
      "\n",
      "    :Date: November, 1995\n",
      "\n",
      "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
      "https://goo.gl/U2Uwz2\n",
      "\n",
      "Features are computed from a digitized image of a fine needle\n",
      "aspirate (FNA) of a breast mass.  They describe\n",
      "characteristics of the cell nuclei present in the image.\n",
      "A few of the images can be found at\n",
      "http://www.cs.wisc.edu/~street/images/\n",
      "\n",
      "Separating plane described above was obtained using\n",
      "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
      "Construction Via Linear Programming.\" Proceedings of the 4th\n",
      "Midwest Artificial Intelligence and Cognitive Science Society,\n",
      "pp. 97-101, 1992], a classification method which uses linear\n",
      "programming to construct a decision tree.  Relevant features\n",
      "were selected using an exhaustive search in the space of 1-4\n",
      "features and 1-3 separating planes.\n",
      "\n",
      "The actual linear program used to obtain the separating plane\n",
      "in the 3-dimensional space is that described in:\n",
      "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
      "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
      "Optimization Methods and Software 1, 1992, 23-34].\n",
      "\n",
      "This database is also available through the UW CS ftp server:\n",
      "\n",
      "ftp ftp.cs.wisc.edu\n",
      "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
      "\n",
      "References\n",
      "----------\n",
      "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
      "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
      "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870, \n",
      "     San Jose, CA, 1993. \n",
      "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
      "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
      "     July-August 1995.\n",
      "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
      "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
      "     163-171.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bc['DESCR']) #uncomment and run to print a description of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Quickly try out different classifiers for the breast cancer dataset. Do the same ones do as well? If not, why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 of 114 correct\n",
      "102 of 114 correct\n",
      "109 of 114 correct\n",
      "104 of 114 correct\n",
      "104 of 113 correct\n",
      " \n",
      "Total: 516 of 569 correct (90.69%)\n"
     ]
    }
   ],
   "source": [
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "import numpy as np\n",
    "#classification libraries\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "#prepare k-fold cross validation\n",
    "kf = KFold(len(bc.target), n_folds=5)\n",
    "KFold(n=4, n_folds=2, shuffle=False, random_state=None)\n",
    "\n",
    "#variables to count up how many we got right\n",
    "tally_correct = 0\n",
    "tally_total = 0\n",
    "for train_index, test_index in kf:\n",
    "    #here we split the dataset up into training and test sets, these change each iteration\n",
    "    training_data = bc.data[train_index,:] \n",
    "    training_target = bc.target[train_index] \n",
    "    test_data = bc.data[test_index,:]\n",
    "    test_target = bc.target[test_index]\n",
    "    \n",
    "    #TODO: Uncomment one of these classifiers to see how it does\n",
    "    #csf = tree.DecisionTreeClassifier()\n",
    "    #csf = ensemble.RandomForestClassifier(n_estimators=10, min_samples_split=1, max_depth=None, max_features=5)\n",
    "    #csf = ensemble.ExtraTreesClassifier(n_estimators=100, min_samples_split=1, max_depth=None, max_features=2)\n",
    "    csf = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "    #csf= svm.LinearSVC(C=1)\n",
    "    #csf = naive_bayes.GaussianNB()\n",
    "    \n",
    "    \n",
    "    csf.fit(training_data,training_target)\n",
    "    \n",
    "    predictions = csf.predict(test_data)\n",
    "    number_correct = np.sum(predictions==test_target)\n",
    "    total_number = len(predictions)\n",
    "    print(\"%d of %d correct\" % (number_correct,total_number))\n",
    "    tally_correct += number_correct\n",
    "    tally_total += total_number\n",
    "print(\" \")\n",
    "print(\"Total: %d of %d correct (%0.2f%%)\" % (tally_correct, tally_total, 100.0*tally_correct/tally_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Exercise**: Look at the other datasets in sklearn, and try out other classifiers on them. What datasets do you know of or have access to which might need classification?\n",
    "\n",
    "**Double Bonus Exercise**: Modify the classifier code above so that it tries different values of the neighbours parameter automatically, and returns the best value of that parameter.\n",
    "\n",
    "**Quick Quiz**\n",
    "\n",
    "How can we avoid 'cheating' when we fit (or 'train') our model?\n",
    "\n",
    "Name two types of classifier.\n",
    "\n",
    "What is over fitting?\n",
    "\n",
    "Name some features of a dataset which make one classifier work better than another?\n",
    "\n",
    "What is the difference between supervised and unsupervised learning?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
